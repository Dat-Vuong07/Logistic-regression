---
title: "Logistic regression"
author: "Dat Quoc Vuong"
date: "4/18/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Multinomial logistic regression

```{r}
# Helper packages
library(dplyr)     # for data wrangling
library(ggplot2)   # for awesome plotting
library(rsample)   # for data splitting

# Modeling packages
library(caret)     # for logistic regression modeling

# Model interpretability packages
library(vip)       # variable importance
```

Example: Entering high school students make program choices among general program, vocational program and academic program. Their choice might be modeled using their writing score and their social economic status.

# Description of the data

```{r, eval=FALSE}
ml <- read.dta("https://stats.idre.ucla.edu/stat/data/hsbdemo.dta")
```

The data set contains variables on 200 students. 
The outcome variable is **prog**, program type. 
The predictor variables are social economic status, **ses**, a three-level categorical variable and writing score, **write**, a continuous variable. 

Let’s start with getting some descriptive statistics of the variables of interest.

```{r, eval=FALSE}
with(ml, table(ses, prog))

with(ml, do.call(rbind, tapply(write, prog, function(x) c(M = mean(x), SD = sd(x)))))
```
## Split data

```{r}
df <- attrition %>% mutate_if(is.ordered, factor, ordered = FALSE)

# Create training (70%) and test (30%) sets for the 
# rsample::attrition data.
set.seed(123)  # for reproducibility
churn_split <- initial_split(df, prop = .7, strata = "Attrition")
churn_train <- training(churn_split)
churn_test  <- testing(churn_split)
```


# Logistic regression

The first predicts the probability of attrition based on their monthly income (MonthlyIncome) and the second is based on whether or not the employee works overtime (OverTime). 

In the background glm() uses ML estimation to estimate the unknown model parameters. In other words, we try to find  
β_0 and β_1 such that plugging these estimates into the model for p (X) (Equation (5.1)) yields a number close to one for all employees who attrited, and a number close to zero for all employees who did not.

What results is the predicted probability of attrition. Figure 5.2 illustrates the predicted probabilities for the two models.

```{r}
model1 <- glm(Attrition ~ MonthlyIncome, family = "binomial", data = churn_train)
model2 <- glm(Attrition ~ OverTime, family = "binomial", data = churn_train)
```

```{r}
broom::tidy(model1)

tidy(model2)
```

**Interprete the out put**

For model1, the estimated coefficient for MonthlyIncome is β_1 = -0.000130, which is negative, indicating that an increase in MonthlyIncome is associated with a decrease in the probability of attrition. To be precise, a one-unit increase in MonthlyIncome is associated with an *decrease* in the log odds of attrition by 0.000130 units.

Similarly, for model2, employees who work OverTime are associated with an increased probability of attrition compared to those that do not work OverTime. To be precise, a one-unit increase in MonthlyIncome is associated with an *increase* in the log odds of attrition by 1.406394 units.

In a logistic regression model, increasing X by one unit changes the log odds by β1 (4.4), or equivalently it multiplies the odds by e^β1 (4.3)

As discussed earlier, it is easier to interpret the coefficients using an exp() transformation:

```{r}

exp(coef(model1))

exp(coef(model2))


```

The quantity p(X)/[1−p(X)] is called the odds, and can take on any value between 0 and ∞. Values of the odds close to 0 and ∞ indicate very low and very high probabilities of default, respectively. 

Thus, the *odds* of an employee attriting in model1 increase multiplicatively by 0.9999 for every one dollar increase in MonthlyIncome, whereas the odds of attriting in model2 increase multiplicatively by 4.0812 for employees that work OverTime compared to those that do not. 

The z-statistic in logitics plays the same role as the t-statistic in the linear regression output. If it significant mean that that the probability of attriting depend on MonthlyIncome.

## Multiple logistic regression

```{r}
model3 <- glm(
  Attrition ~ MonthlyIncome + OverTime,
  family = "binomial", 
  data = churn_train
)

tidy(model3)
```


Our results show that both features are statistically significant (at the 0.05 level) and Figure 5.3 illustrates common trends between MonthlyIncome and Attrition; however, working OverTime tends to nearly double the probability of attrition.

## PLS logistic regression

Similar to linear regression, we can perform a PLS logistic regression to assess if reducing the dimension of our numeric predictors helps to improve accuracy. There are 16 numeric features in our data set so the following code performs a 10-fold cross-validated PLS model while tuning the number of principal components to use from 1–16. The optimal model uses 14 principal components, which is not reducing the dimension by much.


```{r}
# Perform 10-fold CV on a PLS model tuning the number of PCs to 
# use as predictors
set.seed(123)
cv_model_pls <- train(
  Attrition ~ ., 
  data = churn_train, 
  method = "pls",
  family = "binomial",
  trControl = trainControl(method = "cv", number = 10),
  preProcess = c("zv", "center", "scale"),
  tuneLength = 16
)

ggplot(cv_model_pls)

```

## Naive Bayes Classifier

**General Idea**

Suppose that we wish to classify an observation into one of K classes, where K ≥ 2. In other words, the qualitative response variable Y can take on K possible distinct and unordered values. 

We refer to pk(x) as the posterior probability that an observation X = x belongs to the kth class. That is, it is the probability that the observation belongs to the kth class, given the predictor value for that observation.

**Hyperparameters**

We can tune the few hyperparameters that a naïve Bayes model has *usekernel* parameter allows us to use a kernel density estimate for continuous variables versus a guassian density estimate, *adjust* allows us to adjust the bandwidth of the kernel density (larger numbers mean more flexible density estimate), *fL* allows us to incorporate the Laplace smoother.

**Usefull Preprocessing**

By incorporating some preprocessing of our features (normalize with Box Cox, standardize with center-scaling, and reducing with PCA) we actually get about another 2% lift in our accuracy.

**Assumption**

Naive Bayes comes in many forms. With only numeric predictors, it often assumes a multivariate normal conditioned on the classes, but a very specific multivariate normal.

$${\mathbf X} \mid Y = k \sim N(\mu_k, \Sigma_k)$$
Naive Bayes assumes that the predictors $X_1, X_2, \ldots, X_p$ are independent. This is the “naive” part of naive Bayes.
Since $X_1, X_2, \ldots, X_p$ are assumed independent, each $\Sigma_k$ is diagonal, that is, we assume no correlation between predictors. Independence implies zero correlation.

With naïve Bayes, we assume that the predictor variables are conditionally independent of one another given the response value. This is an *extremely strong assumption*. We can see quickly that our attrition data violates this as we have several moderately to strongly correlated variables.

```{r}

churn_train %>%
  filter(Attrition == "Yes") %>%
  select_if(is.numeric) %>%
  cor() %>%
  corrplot::corrplot()

```


For categorical variables, this computation is quite simple as you just use the frequencies from the data. 

However, when including **continuous predictor** variables often an assumption of normality is made so that we can use the probability from the variable’s probability density function. If we pick a handful of our numeric features we quickly see assumption of normality is not always fair.

```{r}
churn_train %>% 
  select(Age, DailyRate, DistanceFromHome, HourlyRate, MonthlyIncome, MonthlyRate) %>% 
  gather(metric, value) %>% 
  ggplot(aes(value, fill = metric)) + 
  geom_density(show.legend = FALSE) + 
  facet_wrap(~ metric, scales = "free")

```

Granted, some numeric features may be normalized with a *Box-Cox transformation*; however, as you will see in this tutorial we can also use non-parametric kernel density estimators to try get a more accurate representation of continuous variable probabilities. Ultimately, transforming the distributions and selecting an estimator is part of the modeling development and tuning process.

**Strength**

The strength of naive Bayes comes from its ability to handle a large number of predictors, p, even with a limited sample size n. Even with the naive independence assumption, naive Bayes works rather well in practice. Also because of this assumption, we can often train naive Bayes where LDA and QDA may be impossible to train because of the large number of parameters relative to the number of observations.

**Weakness**
The greatest weakness of the naïve Bayes classifier is that it relies on an often-faulty assumption of equally important and independent features which results in biased posterior probabilities. Although this assumption is rarely met, in practice, this algorithm works surprisingly well. This is primarily because what is usually needed is not a propensity (exact posterior probability) for each record that is accurate in absolute terms but just a reasonably accurate rank ordering of propensities.

For example, we may not care about the exact posterior probability of attrition, we just want to know for a given observation, is the posterior probability of attriting larger than not attriting. Even when the assumption is violated, the rank ordering of the records’ propensities is typically preserved. Consequentely, naïve Bayes is often a surprisingly accurate algorithm. 

```{r }

# Optional
search_grid <- expand.grid(
  usekernel = c(TRUE, FALSE),
  fL = 0:5,
  adjust = seq(0, 5, by = 1)
)

set.seed(123)
cv_model_BN <- train(
  Attrition ~ ., 
  data = churn_train, 
  method = "nb",
  trControl = trainControl(method = "cv", number = 10),
  preProc = c("BoxCox", "center", "scale", "pca"))

```


## Linear Discriminant Analysis 
*Multiple-class classification*

**Assumption**

In order to estimate fk(x), we will first make some assumptions about its form.

```{r}
set.seed(123)
cv_model_lda <- train(
  Attrition ~ ., 
  data = churn_train, 
  method = "lda",
  family = "binomial",
  trControl = trainControl(method = "cv", number = 10),
  preProcess = c("BoxCox", "zv", "center", "scale"),
  tuneLength = 16
)
```


## Model accuracy


```{r}
set.seed(123)
cv_model1 <- train(
  Attrition ~ MonthlyIncome, 
  data = churn_train, 
  method = "glm",
  family = "binomial",
  trControl = trainControl(method = "cv", number = 10)
)

set.seed(123)
cv_model2 <- train(
  Attrition ~ MonthlyIncome + OverTime, 
  data = churn_train, 
  method = "glm",
  family = "binomial",
  trControl = trainControl(method = "cv", number = 10)
)

set.seed(123)
cv_model3 <- train(
  Attrition ~ ., 
  data = churn_train, 
  method = "glm",
  family = "binomial",
  trControl = trainControl(method = "cv", number = 10)
)
```

```{r}
summary(
  resamples(
    list(
      model1 = cv_model1, 
      model2 = cv_model2, 
      model3 = cv_model3
    )
  )
)$statistics$Accuracy
```

Extracting the accuracy measures (in this case, classification accuracy), we see that both cv_model1 and cv_model2 had an average accuracy of 83.88%. However, cv_model3 which used all predictor variables in our data achieved an average accuracy rate of 87.58%.


## Confusion matrix

We can get a better understanding of our model’s performance by assessing the confusion matrix (see Section 2.6). We can use caret::confusionMatrix() to compute a confusion matrix. 

One thing to point out, in the confusion matrix above you will note the metric No Information Rate: 0.839. This represents the ratio of non-attrition vs. attrition in our training data.

Consequently, if we simply predicted "No" for every employee we would still get an accuracy rate of 83.9%. Therefore, our goal is to maximize our accuracy rate over and above this no information baseline while also trying to balance sensitivity and specificity. To that end, we plot the ROC curve (section 2.6) which is displayed in Figure 5.4. If we compare our simple model (cv_model1) to our full model (cv_model3), we see the lift achieved with the more accurate model.


```{r}
# predict class
pred_class <- predict(cv_model3, churn_train)

# create confusion matrix
confusionMatrix(
  data = relevel(pred_class, ref = "Yes"), 
  reference = relevel(churn_train$Attrition, ref = "Yes")
)

# Metric No Information Rate 

(table(churn_train$Attrition) %>% prop.table())
```






```{r}
library(ROCR)

# Compute predicted probabilities
m1_prob <- predict(cv_model1, churn_train, type = "prob")$Yes
m3_prob <- predict(cv_model3, churn_train, type = "prob")$Yes
pls_prob <- predict(cv_model_pls, churn_train, type = "prob")$Yes
BN_prob <- predict(cv_model_BN2, churn_train, type = "prob")$Yes


# Compute AUC metrics for cv_model1 and cv_model3
perf1 <- prediction(m1_prob, churn_train$Attrition) %>%
  performance(measure = "tpr", x.measure = "fpr")
perf2 <- prediction(m3_prob, churn_train$Attrition) %>%
  performance(measure = "tpr", x.measure = "fpr")
perf.pls <- prediction(pls_prob, churn_train$Attrition) %>%
  performance(measure = "tpr", x.measure = "fpr")
perf.BN <- prediction(BN_prob, churn_train$Attrition) %>%
  performance(measure = "tpr", x.measure = "fpr")

# Plot ROC curves for cv_model1 and cv_model3
plot(perf1, col = "black", lty = 2)
plot(perf2, add = TRUE, col = "blue")
plot(perf.pls, add = TRUE, col = "red")
# plot(perf.lda, add = TRUE, col = "green")
plot(perf.BN, add = TRUE, col = "green")
legend(0.8, 0.2, legend = c("cv_model1", "cv_model3", "clv_pls", "clv_lda"),
       col = c("black", "blue", "red", "green"), lty = 4:1, cex = 0.6)
```

